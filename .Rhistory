gather(topic, proportion, 2:)
doc.topics_t <-
doc.topics %>%
as.data.frame() %>%
tibble::rownames_to_column("fichier") %>%
gather(topic, proportion, 2:11)
View(doc.topics_t)
View(merged)
final <-
doc.topics_t %>%
left_join(merged)
doc.topics_t <-
doc.topics %>%
as.data.frame() %>%
tibble::rownames_to_column("fichier") %>%
gather(topic_id, proportion, 2:11)
View(doc.topics_t)
View(merged)
final <-
doc.topics_t %>%
left_join(merged)
View(final)
View(bigrammes)
View(doc.topics_t)
View(final)
View(labels_tidy)
View(labels_tidy_joined_unigrams)
View(thesaurus)
#Prend le résultat d'un topic modeling effectué avec mallet et ajoute les eurovocs
library(readr)
library(dplyr)
library(tidyr)
library(data.table)
source("get_fingerprint.R") #imitation du fingerprint d'Open Refine
#on importe les labels
dcb_topic_labels <- fread("~/eurovoc_topicmodeling/dcb-topic-labels.csv",
col.names = c("topic_id", "token"),
encoding = "Latin-1")
#ATTENTION : problème avec les étranges caractères si on importe en UTF-8
#la variable "n.topwords" vient du topic modeling (en général 20 labels)
labels <-
dcb_topic_labels %>%
separate(token, into=as.character(1:n.topwords), sep=" ")
#réorganiser topics sous forme de DB, afin de minimiser les matchings ?
labels_tidy <-
melt(labels, id.vars = c("topic_id"),
measure.vars = 2:20) %>%
select(topic_id, value) %>%
arrange(topic_id)
#on importe le thésaurus nettoyé
thesaurus <-
fread("~/eurovoc_topicmodeling/eurovoc_thesaurus_complet.csv", encoding="UTF-8")
#On stemme les tokens (vérifier si ça marche mieux que sans)
#verifier avec un algo moins agressif que porter
# labels_tidy_stem <-
#   labels_tidy[,.(topic_id, value, stem = SnowballC::wordStem(value, language = "porter"))]
######################################
#jointure des unigrammes
######################################
#jointure avec le thesaurus sur les stems
labels_tidy_joined_unigrams <- labels_tidy %>%
fuzzyjoin::stringdist_left_join(thesaurus,
by = c(value = "fingerprint_cleaned"),
distance_col = "dist",
max_dist = 0,
method = "lv") %>%
arrange(as.integer(topic_id), term) %>%
filter(!is.na(term)) %>%
select(topic_id, value, term, french, length, profondeur, category, link, bt1, mt, domain)
# matched <- colMeans(!is.na(labels_tidy_joined))['term']
# print(paste0("Le poucentage de matching est de ", round(matched, 2)))
#essais de summarisation
# df <- labels_tidy_joined %>%
#   group_by(topic_id, mt) %>%
#   summarise(total.domain=n()) %>%
#   arrange(as.integer(topic_id))
#
# View(df)
######################################
#création des bigrammes et fingerprint
######################################
bigrammes <-
melt(setDT(labels_tidy),
id.var = c("topic_id"))[, combn(value, 2, FUN = paste, collapse = ' '), topic_id]
bigrammes <- bigrammes[,.(topic_id, V1, fingerprint = get_fingerprint(V1))]
setnames(bigrammes, "V1", "value")
#on stemme les bigrammes
bigrammes$fingerprint_stem <-  strsplit(bigrammes$fingerprint, " ", fixed = TRUE) %>%
lapply(., function(x) SnowballC::wordStem(x, language = "porter") ) %>%
vapply(., function(x) paste(x, collapse = " "), character(1))
#test de fuzzy join sur une partie des bigrammes
labels_tidy_joined_bigrams <- bigrammes %>%
fuzzyjoin::stringdist_inner_join(thesaurus[length==2],
by = c(fingerprint_stem = "fingerprint_stem"),
distance_col = "dist",
max_dist=0,
ignore_case = TRUE,
method = "lv") %>%
select(topic_id, value, term, french, length, profondeur, category, link, bt1, mt, domain)
######################################
#création des trigrammes et fingerprint
######################################
trigrammes <-
melt(setDT(labels_tidy),
id.var = c("topic_id"))[, combn(value, 3, FUN = paste, collapse = ' '), topic_id]
trigrammes <- trigrammes[,.(topic_id, V1, fingerprint = get_fingerprint(V1))]
setnames(trigrammes, "V1", "value")
#on stemme les bigrammes
trigrammes$fingerprint_stem <-  strsplit(trigrammes$fingerprint, " ", fixed = TRUE) %>%
lapply(., function(x) SnowballC::wordStem(x, language = "porter") ) %>%
vapply(., function(x) paste(x, collapse = " "), character(1))
#test de fuzzy join sur une partie des bigrammes
labels_tidy_joined_trigrams <- trigrammes %>%
fuzzyjoin::stringdist_inner_join(thesaurus[length==3],
by = c(fingerprint_stem = "fingerprint_stem"),
distance_col = "dist",
max_dist=0,
ignore_case = TRUE,
method = "lv") %>%
select(topic_id, value, term, french, length, profondeur, category, link, bt1, mt, domain)
#On merge les unigrams, bigrammes et trigrammes matchés
merged <- rbind(labels_tidy_joined_unigrams,
labels_tidy_joined_bigrams,
labels_tidy_joined_trigrams)
#on réconcilie les topics_id des termes matchés avec la matrice doc.topics transposée
doc.topics_t <-
doc.topics %>%
as.data.frame() %>%
tibble::rownames_to_column("fichier") %>%
gather(topic_id, proportion, 2:11)
final <-
doc.topics_t %>%
left_join(merged)
View(final)
View(final)
somme_cululee <-
final %>%
group_by(fichier, value) %>%
summarise(somme = sum(proportion)) %>%
na.omit() %>%
filter(somme > 0.5) %>%
arrange(desc(fichier), desc(somme)
somme_cumulee <-
final %>%
group_by(fichier, value) %>%
summarise(somme = sum(proportion)) %>%
na.omit() %>%
filter(somme > 0.5) %>%
arrange(desc(fichier), desc(somme))
View(somme_cumulee)
#On retente de faire la somme cumulée des porpotions de chaque token
somme_cumulee <-
final %>%
group_by(fichier, term) %>%
summarise(somme = sum(proportion)) %>%
na.omit() %>%
arrange(desc(fichier), desc(somme))
somme_cumulee <-
final %>%
group_by(fichier, term) %>%
summarise(somme = sum(proportion)*100) %>%
na.omit() %>%
arrange(desc(fichier), desc(somme))
somme_cumulee <-
final %>%
group_by(fichier, term) %>%
summarise(somme = sum(proportion)*100) %>%
filter(somme >= 5)
na.omit() %>%
arrange(desc(fichier), desc(somme))
somme_cumulee <-
final %>%
group_by(fichier, term) %>%
summarise(somme = sum(proportion)*100) %>%
filter(somme >= 5) %>%
arrange(desc(fichier), desc(somme))
somme_cumulee <- somme_cumulee %>%
spread(term, somme)
somme_cumulee <- somme_cumulee %>%
group_by(fichier, topic_id)
spread(term, somme)
somme_cumulee <- somme_cumulee %>%
group_by(fichier, topic_id) %>%
spread(term, somme)
somme_cumulee <-
final %>%
group_by(fichier, term) %>%
summarise(somme = sum(proportion)*100) %>%
filter(somme >= 5)
View(somme_cumulee)
somme_cumulee_spread <- somme_cumulee %>%
group_by(fichier) %>%
spread(term, somme)
View(somme_cumulee_spread)
View(somme_cumulee_spread)
View(somme_cumulee)
somme_cumulee_spread <- somme_cumulee %>%
group_by(fichier) %>%
spread(token, c(term, somme))
getwd()
fwrite(somme_cumulee, "somme_cumulee.csv")
require(mallet)
?mallet.read.dir
jrc_metadata <- fread("~/eurovoc_topicmodeling/jrc_acquis_metadata.csv)
jrc_metadata <- fread("~/eurovoc_topicmodeling/jrc_acquis_metadata.csv")
View(jrc_metadata)
jrc_metadata <- fread("~/eurovoc_topicmodeling/jrc_acquis_metadata.csv") %>%
select(-body)
View(jrc_metadata)
fwrite(jrc_metadata, "jrc_metatada_sansbody.csv")
View(jrc_metadata)
mallet.instances
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jdk1.8.0_131/jre")
options(java.parameters = "-Xmx10g")
#this whole script is based on Ben Marwick's Day of Archaeology work https://github.com/benmarwick/dayofarchaeology
Sys.setenv(NOAWT = TRUE)
require(mallet)
require(dplyr)
#library(tidytext)
library(readr)
#require(qdap) # a tester pour le préprocessing du corpus
#variables principales du topic modeling
iterations= 1000
n.topics <- 100
n.topwords <- 20
#import the documents from the folder
documents <- mallet.read.dir("texts_jrc")
#Processing text : remove digits, (hyphens ?), ?non ascii characters, short words
documents$text <- gsub("[[:digit:]]+", "", documents$text)
#documents$text <- gsub("\\s?-\\s?", "", documents$text)
#cette fonction efface les non ascii, mais tranforme dönut en dnut...
#note : la fonction "latinize" de fingerprint() peut latiniser a posterirori les charactères non-ascii
#documents$text <- iconv(documents$text, "latin1", "ASCII", sub = "")
documents$text <- gsub("\\b\\w{1,2}\\s","", documents$text)
#stem. Très lent, à éviter. Voir si un package en C n'existe pas.
#Note : pas moyen d'utiliser plus d'un mc.cores dans Windows
#Note2 : essayer un algo moins aggressif que Porter
#source("~/eurovoc_topicmodeling/stemming.R")
#documents$text <- stem_text(documents$text, language = 'en', mc.cores = 1)
## Generate and save Stopwords
#Note : objectiver un peu mieux nos choix : tf-idf (voir dataframe word.freqs) ?
#Voir aussi https://mimno.infosci.cornell.edu/papers/schofield_eacl_2017.pdf
#Ajouter des stopwords français, italiens ou allemands ?
stops <- c(
tm::stopwords("english"),
tm::stopwords("SMART"),
"commission",
"shall",
"agreement",
"annex",
"regulation",
"decision",
"article",
"state",
"member",
"european",
"community",
"protocol",
"states",
"council",
"directives",
"directive"
)
write(stops, "~/eurovoc_topicmodeling/stopwords_en.txt")
#create Mallet instances
mallet.instances <-
mallet.import(documents$id,
documents$text,
"~/eurovoc_topicmodeling/stopwords_en.txt",
token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
#tidytext. Voir comment l'adapter
# create a vector with one string per chapter
# collapsed <- documents %>%
#   anti_join(stop_words, by = "word") %>%
#   mutate(word = str_replace(word, "'", "")) %>%
#   group_by(document) %>%
#   summarize(text = paste(word, collapse = " "))
# # create an empty file of "stopwords"
# file.create(empty_file <- tempfile())
# docs <- mallet.import(collapsed$document, collapsed$text,
#                       empty_file)
#create topic trainer object
topic.model <- MalletLDA(n.topics)
#load documents
topic.model$loadDocuments(mallet.instances)
## Get the vocabulary, and some statistics about word frequencies.
## These may be useful in further curating the stopword list.
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
## Optimize hyperparameters every 20 iterations,
## after 50 burn-in iterations.
topic.model$setAlphaOptimization(20, 50)
## Now train a model. Note that hyperparameter optimization is on, by default.
## We can specify the number of iterations.
topic.model$train(iterations)
## NEW: run through a few iterations where we pick the best topic for each token,
## rather than sampling from the posterior distribution.
topic.model$maximize(10)
## Get the probability of topics in documents and the probability of words in topics.
## By default, these functions return raw word counts. Here we want probabilities,
## so we normalize, and add "smoothing" so that nothing has exactly 0 probability.
doc.topics <-
mallet.doc.topics(topic.model, smoothed = T, normalized = T)
topic.words <-
mallet.topic.words(topic.model, smoothed = T, normalized = T)
# UTILISER AUTRE METHODE
# from http://www.cs.princeton.edu/~mimno/R/clustertrees.R
## transpose and normalize the doc topics
# topic.docs <- t(doc.topics)
# topic.docs <- topic.docs / rowSums(topic.docs)
# write.csv(topic.docs, "~/eurovoc_topicmodeling/dcb-topic-docs.csv")
#methode https://txtplorer.wordpress.com/2015/07/23/reshape-mallet-output-r/
# tab <- doc.topics
#
# NTOPICS = 30 # number of topics here
#
# names <- c('num', 'text', paste(c('topic', 'proportion'),
#                                   rep(1:NTOPICS, each = 2), sep = ""))
#
rownames(doc.topics) <- documents$id
colnames(doc.topics) <- as.character(1:n.topics)
## Get a vector containing short names for the topics
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) topics.labels[topic] <-
paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=n.topwords)$words, collapse=" ")
# have a look at keywords for each topic
topics.labels
write.csv(topics.labels,
"~/eurovoc_topicmodeling/dcb-topic-labels.csv")
## Show the first few document titles with at least .25 of its content devoted to topic 4
#head(documents$text[ doc.topics[4,] > 0.25 ],10)
## Show title of the most representative text for topic 4
#documents[which.max(doc.topics[4,]),]$text
# What are the top words in topic 2?
# Notice that R indexes from 1 and Java from 0, so this will be the topic that mallet called topic 1.
#mallet.top.words(topic.model, word.weights = topic.words[2,], num.top.words = 20)
## cluster based on shared words
#plot(hclust(dist(topic.words)), labels = topics.labels)
topics.labels
View(final)
install.packages(c("ape", "car", "caret", "cluster", "ddalpha", "deldir", "emmeans", "ggridges", "ggthemes", "glmnet", "lme4", "lmtest", "maps", "Matrix", "network", "nlme", "proxy", "psych", "Rcmdr", "RcmdrMisc", "rmapshaper", "rotl", "RQDA", "selectr", "spdep", "systemfit", "tint", "XLConnect", "XLConnectJars"))
View(bigrammes)
View(bigrammes)
labels_tidy_joined_bigrams <- bigrammes %>%
fuzzyjoin::stringdist_inner_join(thesaurus,
by = c(fingerprint = "fingerprint_cleaned"),
distance_col = "dist",
max_dist=0,
ignore_case = TRUE,
method = "lv") %>%
select(topic_id, value, term, french, length, profondeur, category, link, bt1, mt, domain)
library(readr)
library(dplyr)
library(tidyr)
library(data.table)
source("get_fingerprint.R") #imitation du fingerprint d'Open Refine
labels_tidy_joined_bigrams <- bigrammes %>%
fuzzyjoin::stringdist_inner_join(thesaurus,
by = c(fingerprint = "fingerprint_cleaned"),
distance_col = "dist",
max_dist=0,
ignore_case = TRUE,
method = "lv") %>%
select(topic_id, value, term, french, length, profondeur, category, link, bt1, mt, domain)
View(labels_tidy_joined_bigrams)
merged <- rbind(labels_tidy_joined_unigrams,
labels_tidy_joined_bigrams)
merged <- rbind(labels_tidy_joined_unigrams,
labels_tidy_joined_bigrams)
View(merged)
View(labels_tidy_joined_unigrams)
View(labels_tidy_joined_unigrams)
library(readr)
library(dplyr)
library(tidyr)
library(data.table)
source("get_fingerprint.R") #imitation du fingerprint d'Open Refine
topic_labels <- fread("ordered_topics.csv",
header= FALSE,
skip=1,
encoding = "Latin-1")
topic_labels
labels_tidy <-
melt(topic_labels, id.vars = c("V1"),
measure.vars = 2:ncol(topic_labels)) %>%
select(topic_id=V1, tokens=value) %>%
arrange(topic_id)
labels_tidy
thesaurus <-
fread("eurovoc_thesaurus_complet.csv", encoding="UTF-8")
View(thesaurus)
labels_tidy_joined_unigrams <- labels_tidy %>%
fuzzyjoin::stringdist_left_join(thesaurus,
by = c(tokens = "fingerprint_cleaned"),
distance_col = "dist",
max_dist = 0,
method = "lv") %>%
arrange(as.integer(topic_id), term) %>%
filter(!is.na(term)) %>%
select(topic_id, tokens, term, french, length, profondeur, category, link, bt1, mt, domain)
View(labels_tidy_joined_unigrams )
bigrammes <-
melt(setDT(labels_tidy),
id.var = c("topic_id"))[, combn(value, 2, FUN = paste, collapse = ' '), topic_id]
bigrammes <- bigrammes[,.(topic_id, V1, fingerprint = get_fingerprint(V1))]
setnames(bigrammes, "V1", "value")
View(bigrammes)
labels_tidy_joined_bigrams <- bigrammes %>%
fuzzyjoin::stringdist_inner_join(thesaurus,
by = c(fingerprint = "fingerprint_cleaned"),
distance_col = "dist",
max_dist=0,
ignore_case = TRUE,
method = "lv") %>%
select(topic_id, value, term, french, length, profondeur, category, link, bt1, mt, domain)
labels_tidy_joined_bigrams <- bigrammes %>%
fuzzyjoin::stringdist_inner_join(thesaurus,
by = c(fingerprint = "fingerprint_cleaned"),
distance_col = "dist",
max_dist=0,
ignore_case = TRUE,
method = "lv") %>%
select(topic_id, value, term, french, length, profondeur, category, link, bt1, mt, domain)
View(labels_tidy_joined_bigrams)
merged <- rbind(labels_tidy_joined_unigrams,
labels_tidy_joined_bigrams)
View(labels_tidy_joined_bigrams)
View(labels_tidy_joined_unigrams)
start.time <- Sys.time()
labels_tidy_joined_bigrams <- bigrammes %>%
fuzzyjoin::stringdist_inner_join(thesaurus,
by = c(fingerprint = "fingerprint_cleaned"),
distance_col = "dist",
max_dist=0,
ignore_case = TRUE,
method = "lv") %>%
select(topic_id, tokens = value, term, french, length, profondeur, category, link, bt1, mt, domain)
end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken
View(labels_tidy_joined_bigrams)
merged <- rbind(labels_tidy_joined_unigrams,
labels_tidy_joined_bigrams)
View(merged)
merged <- rbind(labels_tidy_joined_unigrams,
labels_tidy_joined_bigrams) %>% arrange(desc(topic_id))
View(merged)
fwrite(merged, file = "merged_mathias.csv")
View(labels_tidy)
merged <- rbind(labels_tidy_joined_unigrams,
labels_tidy_joined_bigrams) %>% right_join(labels_tidy, by=c("topic_id", "tokens"))
View(merged)
View(bigrammes)
View(labels_tidy_joined_bigrams)
merged <- rbind(labels_tidy_joined_unigrams,
labels_tidy_joined_bigrams)
View(merged)
fwrite(merged, file = "merged_mathias.csv")
library(readr)
library(dplyr)
library(tidyr)
library(data.table)
source("get_fingerprint.R") #imitation du fingerprint d'Open Refine
topic_labels <- fread("ordered_topics.csv",
header= FALSE,
skip=1,
encoding = "Latin-1")
topic_labels
labels_tidy <-
melt(topic_labels, id.vars = c("V1"),
measure.vars = 2:ncol(topic_labels)) %>%
select(topic_id=V1, tokens=value)
labels_tidy
labels_tidy <-
melt(topic_labels, id.vars = c("V1"),
measure.vars = 2:ncol(topic_labels)) %>%
select(topic_id=V1, tokens=value)
labels_tidy
labels_tidy <-
labels_tidy %>%
group_by(topic_id) %>% mutate(token_id = row_number(), topic_number = stringr::str_extract(topic_id, "\\d+")) %>%
ungroup()
labels_tidy
thesaurus <-
fread("eurovoc_thesaurus_complet.csv", encoding="UTF-8")
thesaurus
labels_tidy_joined_unigrams <- labels_tidy %>%
fuzzyjoin::stringdist_left_join(thesaurus,
by = c(tokens = "fingerprint_cleaned"),
distance_col = "dist",
max_dist = 0,
method = "lv") %>%
arrange(as.integer(topic_id), term) %>%
filter(!is.na(term)) %>%
select(topic_id, token_id, topic_number, tokens, term, french, length, profondeur, category, link, bt1, mt, domain)
labels_tidy_joined_unigrams
bigrammes <-
melt(setDT(labels_tidy),
id.var = c("topic_id"))[, combn(value, 2, FUN = paste, collapse = ' '), topic_id]
bigrammes <- bigrammes[,.(topic_id, V1, fingerprint = get_fingerprint(V1))]
setnames(bigrammes, "V1", "value")
bigrammes
labels_tidy_joined_unigrams <- labels_tidy %>%
fuzzyjoin::stringdist_left_join(thesaurus,
by = c(tokens = "fingerprint_cleaned"),
distance_col = "dist",
max_dist = 0,
method = "lv") %>%
arrange(as.integer(topic_id), term) %>%
filter(!is.na(term)) %>%
select(topic_id, token_id, topic_number, tokens, term, dist, french, length, profondeur, category, link, bt1, mt, domain)
labels_tidy_joined_unigrams
